{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KzUkhkXrVnUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0639e6-7523-4b89-dbe7-dbf8185c3f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading & Random Augmentation"
      ],
      "metadata": {
        "id": "EIqEnmca7lVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NuIJaU3PW9io"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def randomHueSaturationValue(image, hue_shift_limit=(-180, 180),\n",
        "                             sat_shift_limit=(-255, 255),\n",
        "                             val_shift_limit=(-255, 255), u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        h, s, v = cv2.split(image)\n",
        "        hue_shift = np.random.uniform(hue_shift_limit[0], hue_shift_limit[1])\n",
        "        h = cv2.add(h, hue_shift)\n",
        "        sat_shift = np.random.uniform(sat_shift_limit[0], sat_shift_limit[1])\n",
        "        s = cv2.add(s, sat_shift)\n",
        "        val_shift = np.random.uniform(val_shift_limit[0], val_shift_limit[1])\n",
        "        v = cv2.add(v, val_shift)\n",
        "        image = cv2.merge((h, s, v))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def randomShiftScaleRotate(image, mask,\n",
        "                           shift_limit=(-0.0, 0.0),\n",
        "                           scale_limit=(-0.0, 0.0),\n",
        "                           rotate_limit=(-0.0, 0.0),\n",
        "                           aspect_limit=(-0.0, 0.0),\n",
        "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        # print(\"ShiftScaleRotate\")\n",
        "        height, width, channel = image.shape\n",
        "\n",
        "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])\n",
        "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
        "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
        "        sx = scale * aspect / (aspect ** 0.5)\n",
        "        sy = scale / (aspect ** 0.5)\n",
        "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
        "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
        "\n",
        "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
        "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
        "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
        "\n",
        "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
        "        box1 = box0 - np.array([width / 2, height / 2])\n",
        "        box1 = np.dot(box1, rotate_matrix.T) + \\\n",
        "            np.array([width / 2 + dx, height / 2 + dy])\n",
        "\n",
        "        box0 = box0.astype(np.float32)\n",
        "        box1 = box1.astype(np.float32)\n",
        "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
        "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
        "                                    borderValue=(\n",
        "                                        0, 0,\n",
        "                                        0,))\n",
        "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
        "                                   borderValue=(\n",
        "                                       0, 0,\n",
        "                                       0,))\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def randomHorizontalFlip(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        # print(\"HorizontalFlip\")\n",
        "        image = cv2.flip(image, 1)\n",
        "        mask = cv2.flip(mask, 1)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def randomVerticleFlip(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        # print(\"VerticleFlip\")\n",
        "        image = cv2.flip(image, 0)\n",
        "        mask = cv2.flip(mask, 0)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def randomRotate90(image, mask, u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        # print(\"Rotate90\")\n",
        "        image = np.rot90(image)\n",
        "        mask = np.rot90(mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def default_loader(filename, image_root, gt_root, resize_shape):\n",
        "    img = cv2.imread(os.path.join(image_root, filename))\n",
        "    mask = cv2.imread(os.path.join(gt_root, filename), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # the network need the size to be a multiple of 32, resize is intriduced\n",
        "    img = cv2.resize(img, resize_shape)\n",
        "    mask = cv2.resize(mask, resize_shape)\n",
        "\n",
        "    img = randomHueSaturationValue(img,\n",
        "                                   hue_shift_limit=(-20, 20),\n",
        "                                   sat_shift_limit=(-10, 10),\n",
        "                                   val_shift_limit=(-10, 10))\n",
        "\n",
        "    img, mask = randomShiftScaleRotate(img, mask,\n",
        "                                       shift_limit=(-0, 0),\n",
        "                                       scale_limit=(-0.1, 0.1),\n",
        "                                       aspect_limit=(-0.1, 0.1),\n",
        "                                       rotate_limit=(-180, 180))\n",
        "    img, mask = randomHorizontalFlip(img, mask)\n",
        "    img, mask = randomVerticleFlip(img, mask)\n",
        "    img, mask = randomRotate90(img, mask)\n",
        "\n",
        "    mask = np.expand_dims(mask, axis=2)\n",
        "    img = np.array(img, np.float32).transpose(2, 0, 1)/255.0 * 3.2 - 1.6\n",
        "    mask = np.array(mask, np.float32).transpose(2, 0, 1)/255.0\n",
        "    mask[mask >= 0.5] = 1\n",
        "    mask[mask <= 0.5] = 0\n",
        "    # mask = abs(mask-1)\n",
        "    return img, mask\n",
        "\n",
        "\n",
        "class ImageFolder(data.Dataset):\n",
        "\n",
        "    def __init__(self, trainlist, image_root, gt_root, resize_shape):\n",
        "        self.ids = trainlist\n",
        "        self.loader = default_loader\n",
        "        self.image_root = image_root\n",
        "        self.gt_root = gt_root\n",
        "        self.resize_shape = resize_shape\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.ids[index]\n",
        "        img, mask = self.loader(filename, self.image_root,\n",
        "                                self.gt_root, self.resize_shape)\n",
        "        img = torch.Tensor(img)\n",
        "        mask = torch.Tensor(mask)\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intitializing the Training Frame to work on:"
      ],
      "metadata": {
        "id": "2YAMFvVO7zfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We here Introduced th Functions that we will use , and initialzed the learning rate and how to update it  , how to save the model ,  how to optimize the model also computing the F1 score"
      ],
      "metadata": {
        "id": "83kNjJ8U77b4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fdXIRy-bSTZt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable as V\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MyFrame():\n",
        "    def __init__(self, net, loss, lr=8e-4, evalmode=False):\n",
        "        self.device = torch.device(\n",
        "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.net = net().to(self.device)\n",
        "        # self.net = torch.nn.DataParallel(self.net, device_ids=range(torch.cuda.device_count()))\n",
        "        self.optimizer = torch.optim.Adam(params=self.net.parameters(), lr=lr)\n",
        "        #self.optimizer = torch.optim.RMSprop(params=self.net.parameters(), lr=lr)\n",
        "\n",
        "        self.loss = loss()\n",
        "        self.old_lr = lr\n",
        "        if evalmode:\n",
        "            for i in self.net.modules():\n",
        "                if isinstance(i, nn.BatchNorm2d):\n",
        "                    i.eval()\n",
        "\n",
        "    def set_input(self, img_batch, mask_batch=None, img_id=None):\n",
        "        self.img = img_batch\n",
        "        self.mask = mask_batch\n",
        "        self.img_id = img_id\n",
        "\n",
        "    def test_one_img(self, img):\n",
        "        pred = self.net.forward(img)\n",
        "\n",
        "        pred[pred > 0.5] = 1\n",
        "        pred[pred <= 0.5] = 0\n",
        "        mask = pred.squeeze().cpu().data.numpy()\n",
        "        return mask\n",
        "\n",
        "    def test_batch(self):\n",
        "        self.forward(volatile=True)\n",
        "        mask = self.net.forward(self.img).cpu().data.numpy().squeeze(1)\n",
        "        mask[mask > 0.5] = 1\n",
        "        mask[mask <= 0.5] = 0\n",
        "\n",
        "        return mask, self.img_id\n",
        "\n",
        "    def test_one_img_from_path(self, path):\n",
        "        img = cv2.imread(path)\n",
        "        img = np.array(img, np.float32)/255.0 * 3.2 - 1.6\n",
        "        # img = V(torch.Tensor(img).cuda())\n",
        "        img = V(torch.Tensor(img).to(self.device))\n",
        "\n",
        "        mask = self.net.forward(img).squeeze(\n",
        "        ).cpu().data.numpy()\n",
        "        mask[mask > 0.5] = 1\n",
        "        mask[mask <= 0.5] = 0\n",
        "        return mask\n",
        "\n",
        "    def forward(self, volatile=False):\n",
        "        self.img = V(self.img.to(self.device), volatile=volatile)\n",
        "        if self.mask is not None:\n",
        "            self.mask = V(self.mask.to(self.device), volatile=volatile)\n",
        "\n",
        "    def optimize(self, eval=False):\n",
        "        self.forward()\n",
        "        if not eval:\n",
        "            self.optimizer.zero_grad()\n",
        "            self.net.train()\n",
        "            pred = self.net.forward(self.img)\n",
        "            loss = self.loss(self.mask, pred)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        else:\n",
        "            self.net.eval()\n",
        "            pred = self.net.forward(self.img)\n",
        "            loss = self.loss(self.mask, pred)\n",
        "        pred_made = torch.clone(pred)\n",
        "        pred_made[pred_made > 0.5] = 1\n",
        "        pred_made[pred_made <= 0.5] = 0\n",
        "        F1 = self.compute_F1(self.mask, pred_made)\n",
        "\n",
        "        return F1, loss.item()\n",
        "\n",
        "    def compute_F1(self, gt, pred):\n",
        "        \"\"\"extract label list\"\"\"\n",
        "        f1 = f1_score(torch.ravel(gt).cpu().detach().numpy(), \\\n",
        "            torch.ravel(pred).cpu().detach().numpy(), zero_division = 0)\n",
        "        return f1\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.net.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.net.load_state_dict(torch.load(path))\n",
        "\n",
        "    def update_lr(self, new_lr, mylog, factor=False):\n",
        "        if factor:\n",
        "            new_lr = self.old_lr / new_lr\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "        print(mylog, 'update learning rate: %f -> %f' % (self.old_lr, new_lr))\n",
        "        print('update learning rate: %f -> %f' % (self.old_lr, new_lr))\n",
        "        self.old_lr = new_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Losses that we have used"
      ],
      "metadata": {
        "id": "inx5Zu238g08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have combined between the BCE and Dice loss to have more stabile learning and to deal with problem of imbalance classes"
      ],
      "metadata": {
        "id": "bnOq7h4N80yE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FGOp2mwqVDTL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable as V\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class dice_bce_loss(nn.Module):\n",
        "    def __init__(self, batch=True):\n",
        "        super(dice_bce_loss, self).__init__()\n",
        "        self.batch = batch\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    def soft_dice_coeff(self, y_true, y_pred):\n",
        "        smooth = 0.5\n",
        "        if self.batch:\n",
        "            i = torch.sum(y_true)\n",
        "            j = torch.sum(y_pred)\n",
        "            intersection = torch.sum(y_true * y_pred)\n",
        "        else:\n",
        "            i = y_true.sum(1).sum(1).sum(1)\n",
        "            j = y_pred.sum(1).sum(1).sum(1)\n",
        "            intersection = (y_true * y_pred).sum(1).sum(1).sum(1)\n",
        "        score = (2. * intersection + smooth) / (i + j + smooth)\n",
        "        return score.mean()\n",
        "\n",
        "    def soft_dice_loss(self, y_true, y_pred):\n",
        "        loss = 1 - self.soft_dice_coeff(y_true, y_pred)\n",
        "        return loss\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        a = self.bce_loss(y_pred, y_true)\n",
        "        b = self.soft_dice_loss(y_true, y_pred)\n",
        "        return a + b\n",
        "\n",
        "class bce_loss(nn.Module):\n",
        "    def __init__(self, batch=True):\n",
        "        super(bce_loss, self).__init__()\n",
        "        self.batch = batch\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        loss = self.bce_loss(y_pred, y_true)\n",
        "        return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L1b0qiCGJ7-E"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('/content/drive//gith')\n",
        "sys.path.append('/content/drive/My Drive/gith')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Frame"
      ],
      "metadata": {
        "id": "Ng_eO5o49ak1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AYJNVOrKXw43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "\n",
        "from networks.dinknet import LinkNet34, DinkNet34, DinkNet50, DinkNet101, DinkNet152, DinkNet34_less_pool\n",
        "\n",
        "\n",
        "class TTAFrame():\n",
        "    def __init__(self, net):\n",
        "        self.device = torch.device(\n",
        "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.net = net().to(self.device)\n",
        "\n",
        "    def test_one_img_from_path(self, path, evalmode=True):\n",
        "        if evalmode:\n",
        "            self.net.eval()\n",
        "        return self.test_one_img_from_path_2(path)\n",
        "\n",
        "    def test_one_img_from_path_2(self, path):\n",
        "        img = cv2.imread(path)\n",
        "        img90 = np.array(np.rot90(img))\n",
        "        img1 = np.concatenate([img[None], img90[None]])\n",
        "        img2 = np.array(img1)[:, ::-1]\n",
        "        img3 = np.concatenate([img1, img2])\n",
        "        img4 = np.array(img3)[:, :, ::-1]\n",
        "        img5 = img3.transpose(0, 3, 1, 2)\n",
        "        img5 = np.array(img5, np.float32)/255.0 * 3.2 - 1.6\n",
        "        img5 = V(torch.Tensor(img5).to(self.device))\n",
        "        img6 = img4.transpose(0, 3, 1, 2)\n",
        "        img6 = np.array(img6, np.float32)/255.0 * 3.2 - 1.6\n",
        "        img6 = V(torch.Tensor(img6).to(self.device))\n",
        "\n",
        "        maska = self.net.forward(img5).squeeze(\n",
        "        ).cpu().data.numpy()  # .squeeze(1)\n",
        "        maskb = self.net.forward(img6).squeeze().cpu().data.numpy()\n",
        "\n",
        "        mask1 = maska + maskb[:, :, ::-1]\n",
        "        mask2 = mask1[:2] + mask1[2:, ::-1]\n",
        "        mask3 = mask2[0] + np.rot90(mask2[1])[::-1, ::-1]\n",
        "\n",
        "        return mask3\n",
        "\n",
        "    def load(self, path):\n",
        "        if torch.cuda.is_available():\n",
        "            self.net.load_state_dict(torch.load(path))\n",
        "        else:\n",
        "            self.net.load_state_dict(torch.load(\n",
        "                path, map_location=self.device))\n",
        "\n",
        "\n",
        "def test():\n",
        "    source_root = '/content/drive/My Drive/gith/dataset/test_set_images'\n",
        "    folder_names = sorted(os.listdir(source_root))\n",
        "    img_names = [str(i)+'.png' for i in folder_names]\n",
        "    solver = TTAFrame(DinkNet152)\n",
        "    #  Load weights learned from training phase\n",
        "    solver.load('weights/best_DinkNet152_8_2e_4.th')\n",
        "    tic = time()\n",
        "    #  The path that stores the resulting mask of test set\n",
        "    target = 'submits/DinkNet152/'\n",
        "    os.makedirs(target, exist_ok=True)\n",
        "    for i, name in enumerate(img_names):\n",
        "        if (name == '.DS_Store'):\n",
        "            continue\n",
        "        if i % 10 == 0:\n",
        "            print(i/10, '    ', '%.2f' % (time()-tic))\n",
        "        print(\"Testing on \", name)\n",
        "        path = os.path.join(source_root, folder_names[i], name)\n",
        "        mask = solver.test_one_img_from_path(path)\n",
        "        # Here we use threshold = 0.5 for each image, for 8 images batch, it is 4.0(mean)\n",
        "        mask[mask > 4.0] = 255\n",
        "        mask[mask <= 4.0] = 0\n",
        "        cv2.imwrite(target+name[:-4]+'_mask.png', mask.astype(np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Model"
      ],
      "metadata": {
        "id": "wXta9tZN-3zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "nonlinearity = partial(F.relu, inplace=True)\n",
        "\n",
        "\n",
        "class Dblock_more_dilate(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(Dblock_more_dilate, self).__init__()\n",
        "        self.dilate1 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=1, padding=1)\n",
        "        self.dilate2 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=2, padding=2)\n",
        "        self.dilate3 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=4, padding=4)\n",
        "        self.dilate4 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=8, padding=8)\n",
        "        self.dilate5 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=16, padding=16)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        dilate1_out = nonlinearity(self.dilate1(x))\n",
        "        dilate2_out = nonlinearity(self.dilate2(dilate1_out))\n",
        "        dilate3_out = nonlinearity(self.dilate3(dilate2_out))\n",
        "        dilate4_out = nonlinearity(self.dilate4(dilate3_out))\n",
        "        dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
        "        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out + dilate5_out\n",
        "        return out\n",
        "\n",
        "\n",
        "class Dblock(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(Dblock, self).__init__()\n",
        "        self.dilate1 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=1, padding=1)\n",
        "        self.dilate2 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=2, padding=2)\n",
        "        self.dilate3 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=4, padding=4)\n",
        "        self.dilate4 = nn.Conv2d(\n",
        "            channel, channel, kernel_size=3, dilation=8, padding=8)\n",
        "        #self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        dilate1_out = nonlinearity(self.dilate1(x))\n",
        "        dilate2_out = nonlinearity(self.dilate2(dilate1_out))\n",
        "        dilate3_out = nonlinearity(self.dilate3(dilate2_out))\n",
        "        dilate4_out = nonlinearity(self.dilate4(dilate3_out))\n",
        "        #dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
        "        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
        "        self.relu1 = nonlinearity\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(\n",
        "            in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
        "        self.relu2 = nonlinearity\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
        "        self.relu3 = nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class DinkNet152(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DinkNet152, self).__init__()\n",
        "\n",
        "        filters = [256, 512, 1024, 2048]\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "\n",
        "        self.dblock = Dblock_more_dilate(2048)\n",
        "\n",
        "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
        "\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.finalrelu1 = nonlinearity\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.finalrelu2 = nonlinearity\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "\n",
        "        # Center\n",
        "        e4 = self.dblock(e4)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "        out = self.finaldeconv1(d1)\n",
        "        out = self.finalrelu1(out)\n",
        "        out = self.finalconv2(out)\n",
        "        out = self.finalrelu2(out)\n",
        "        out = self.finalconv3(out)\n",
        "\n",
        "        return torch.sigmoid(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "J8IInOP4-8HE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fqa0ixypaxYX"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('logs'):\n",
        "    os.makedirs('logs')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NfJkmAYjbWyM"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('weights'):\n",
        "    os.makedirs('weights')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "sM-DTi7F_zBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "\n",
        "from loss import dice_bce_loss, bce_loss, JaccLoss\n",
        "from data import ImageFolder\n",
        "\n",
        "\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import re\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "import random\n",
        "import math\n",
        "from time import time\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the augmentation function\n",
        "def augment_image(image, mask):\n",
        "    transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.RandomRotate90(p=0.3),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.4),\n",
        "        A.PadIfNeeded(min_height=384, min_width=384, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "        ToTensorV2()  # This converts to PyTorch tensor\n",
        "    ])\n",
        "    augmented = transform(image=image, mask=mask)\n",
        "    return augmented['image'], augmented['mask']\n",
        "\n",
        "# Calculate F1 score\n",
        "def calculate_f1(y_pred, y_true, threshold=0.5):\n",
        "    y_pred = (y_pred > threshold).float().view(-1).cpu().numpy()\n",
        "    y_true = y_true.view(-1).cpu().numpy()\n",
        "    return f1_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "# Initialize variables\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "ORIG_SHAPE = (400, 400)\n",
        "SHAPE = (384, 384)\n",
        "NAME = 'DinkNet152_8_2e_4'\n",
        "BATCHSIZE_PER_CARD = 8\n",
        "train_root = \"/content/drive/My Drive/gith/dataset/training\"\n",
        "\n",
        "image_root = os.path.join(train_root, 'images')\n",
        "gt_root = os.path.join(train_root, 'groundtruth')\n",
        "image_list = np.array(sorted([f for f in os.listdir(image_root) if f.endswith('.png')]))\n",
        "gt_list = np.array(sorted([f for f in os.listdir(gt_root) if f.endswith('.png')]))\n",
        "\n",
        "# Randomly select 5% of training data for validation\n",
        "total_data_num = image_list.shape[0]\n",
        "validation_data_num = math.ceil(total_data_num * 0.05)\n",
        "validation_idx = random.sample(range(total_data_num), validation_data_num)\n",
        "new_train_indx = list(set(range(total_data_num)).difference(set(validation_idx)))\n",
        "\n",
        "val_img_list = image_list[validation_idx].tolist()\n",
        "val_gt_list = gt_list[validation_idx].tolist()\n",
        "image_list = image_list[new_train_indx].tolist()\n",
        "gt_list = gt_list[new_train_indx].tolist()\n",
        "\n",
        "solver = MyFrame(DinkNet152, dice_bce_loss, 8e-4) ########################################### MODEL learning rate\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
        "    val_batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
        "else:\n",
        "    train_batchsize = BATCHSIZE_PER_CARD\n",
        "    val_batchsize = BATCHSIZE_PER_CARD\n",
        "\n",
        "train_dataset = ImageFolder(image_list, image_root, gt_root, SHAPE)\n",
        "val_dataset = ImageFolder(val_img_list, image_root, gt_root, SHAPE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batchsize, shuffle=True, num_workers=0)\n",
        "\n",
        "mylog = open('logs/' + NAME + '.log', 'w')\n",
        "tic = time()\n",
        "no_optim = 0\n",
        "total_epoch = 120\n",
        "train_epoch_best_loss = 100.\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "train_F1_list = []\n",
        "val_F1_list = []\n",
        "best_val_F1 = 0\n",
        "\n",
        "thresh = 0.85 ############################## f1 threshold\n",
        "outer_loop = 8 ######################### outer loop\n",
        "epochs_per_loop = 15 ######################### inner loop\n",
        "############################################# Total loops = 120.   while 15 * 8 = 120\n",
        "\n",
        "for i in range(outer_loop):\n",
        "  if i == 2:\n",
        "    thresh = 0.90\n",
        "    print(\"Threshold changed in stage 2 , set to 0.90\")\n",
        "  if i == 6:\n",
        "    thresh = 0.94\n",
        "    print(\"Threshold changed in stage 6 , set to 0.94\")\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(1, epochs_per_loop + 1):\n",
        "      print('---------- Epoch:' + str(i*epochs_per_loop + epoch) + ' ----------')\n",
        "      train_epoch_loss = 0\n",
        "      train_epoch_F1 = 0\n",
        "      low_f1_images = []\n",
        "      low_f1_masks = []\n",
        "\n",
        "      print('Train:')\n",
        "      for img, mask in train_loader:\n",
        "          solver.set_input(img, mask)\n",
        "          train_F1, train_loss = solver.optimize()\n",
        "          train_epoch_loss += train_loss\n",
        "          train_epoch_F1 += train_F1\n",
        "\n",
        "          # Collect images with low F1 scores for augmentation\n",
        "          if train_F1 < thresh:\n",
        "              for img, mask in zip(img, mask):\n",
        "                  low_f1_images.append(img.cpu())\n",
        "                  low_f1_masks.append(mask.cpu())\n",
        "\n",
        "      train_epoch_loss /= len(train_loader)\n",
        "      train_epoch_F1 /= len(train_loader)\n",
        "\n",
        "      duration_of_epoch = int(time() - tic)\n",
        "\n",
        "      train_F1_list.append(train_epoch_F1)\n",
        "      train_loss_list.append(train_epoch_loss)\n",
        "\n",
        "      mylog.write('********************' + '\\n')\n",
        "      mylog.write('--epoch:' + str(i*epochs_per_loop + epoch) + '  --time:' + str(duration_of_epoch) + '  --train_loss:' + str(\n",
        "          train_epoch_loss) + '  --train_F1:' + str(train_epoch_F1) + '\\n')\n",
        "      print('--epoch:', i*epochs_per_loop + epoch, '  --time:', duration_of_epoch, '  --train_loss:',\n",
        "            train_epoch_loss, '  --train_F1:', train_epoch_F1)\n",
        "\n",
        "      # Validation\n",
        "      val_epoch_loss = 0\n",
        "      val_epoch_F1 = 0\n",
        "      print(\"Validation: \")\n",
        "\n",
        "      for val_img, val_mask in val_loader:\n",
        "          solver.set_input(val_img, val_mask)\n",
        "          val_F1, val_loss = solver.optimize(True)\n",
        "          val_epoch_loss += val_loss\n",
        "          val_epoch_F1 += val_F1\n",
        "\n",
        "      val_epoch_loss /= len(val_loader)\n",
        "      val_epoch_F1 /= len(val_loader)\n",
        "\n",
        "      val_loss_list.append(val_epoch_loss)\n",
        "      val_F1_list.append(val_epoch_F1)\n",
        "\n",
        "      mylog.write('--epoch:' + str(i*epochs_per_loop + epoch) + '  --validation_loss:' + str(val_epoch_loss) + '  --validation_F1:' + str(\n",
        "          val_epoch_F1) + '\\n')\n",
        "      print('--epoch:', i*epochs_per_loop + epoch,  '  --validation_loss:', val_epoch_loss, '  --validation_F1:', val_epoch_F1)\n",
        "\n",
        "      # Save the model with the best validation F1 score\n",
        "      if val_epoch_F1 > best_val_F1:\n",
        "          best_val_F1 = val_epoch_F1\n",
        "          solver.save('weights/best_' + NAME + '.th')\n",
        "          print(f\"Saved best model with F1: {best_val_F1:.4f}\")\n",
        "\n",
        "      if train_epoch_loss >= train_epoch_best_loss:\n",
        "          no_optim += 1\n",
        "      else:\n",
        "          no_optim = 0\n",
        "          train_epoch_best_loss = train_epoch_loss\n",
        "          solver.save('weights/' + NAME + '.th')\n",
        "\n",
        "      if no_optim > 10:\n",
        "          mylog.write('early stop at' + str(epoch) + 'epoch')\n",
        "          print('early stop at %d epoch' % epoch)\n",
        "          break\n",
        "      if no_optim > 3 and solver.old_lr >= 5e-7:\n",
        "          # solver.load('weights/' + NAME + '.th')\n",
        "          solver.update_lr(1.5, factor=True, mylog=mylog)\n",
        "\n",
        "      # Augment low F1 score images and add them to the training set\n",
        "      augmented_images = []\n",
        "      augmented_masks = []\n",
        "      for img, mask in zip(low_f1_images, low_f1_masks):\n",
        "          augmented_img, augmented_mask = augment_image(img.numpy().transpose(1, 2, 0), mask.numpy().transpose(1, 2, 0))\n",
        "          augmented_images.append(augmented_img.cpu())\n",
        "          augmented_masks.append(augmented_mask.cpu().permute(2, 0, 1))\n",
        "\n",
        "      if augmented_images and epoch == epochs_per_loop:\n",
        "          augmented_images = torch.stack(augmented_images)\n",
        "          augmented_masks = torch.stack(augmented_masks)\n",
        "          augmented_dataset = TensorDataset(augmented_images, augmented_masks)\n",
        "\n",
        "          # Concatenate the original dataset with the augmented dataset\n",
        "          new_train_dataset = ConcatDataset([train_loader.dataset, augmented_dataset])\n",
        "\n",
        "          print(f\"Augmented {len(augmented_dataset)} images and added to the training set.\")\n",
        "\n",
        "          # Reinitialize the DataLoader with the new dataset\n",
        "          train_loader = DataLoader(new_train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=0)\n",
        "\n",
        "mylog.write('--complete_train_loss:' + str(train_loss_list) + '\\n')\n",
        "mylog.write('--complete_validation_loss:' + str(val_loss_list) + '\\n')\n",
        "mylog.write('--complete_train_F1_scores:' + str(train_F1_list) + '\\n')\n",
        "mylog.write('--complete_validation_F1_scores:' + str(val_F1_list) + '\\n')\n",
        "mylog.close()\n",
        "\n",
        "print('Finish!')\n",
        "mylog.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsI6ISfA_1Vl",
        "outputId": "33da280e-2147-480f-d3f6-b9ea164f97c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 167MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Epoch:1 ----------\n",
            "Train:\n",
            "--epoch: 1   --time: 216   --train_loss: 1.2751744786898296   --train_F1: 0.03659347962298955\n",
            "Validation: \n",
            "--epoch: 1   --validation_loss: 1.4353443384170532   --validation_F1: 0.0\n",
            "---------- Epoch:2 ----------\n",
            "Train:\n",
            "--epoch: 2   --time: 257   --train_loss: 0.9565448661645254   --train_F1: 0.0002395470027192264\n",
            "Validation: \n",
            "--epoch: 2   --validation_loss: 21.005945205688477   --validation_F1: 0.0\n",
            "---------- Epoch:3 ----------\n",
            "Train:\n",
            "--epoch: 3   --time: 286   --train_loss: 0.8279686570167542   --train_F1: 0.6734149937755158\n",
            "Validation: \n",
            "--epoch: 3   --validation_loss: 4.7367448806762695   --validation_F1: 0.2182166590665717\n",
            "Saved best model with F1: 0.2182\n",
            "---------- Epoch:4 ----------\n",
            "Train:\n",
            "--epoch: 4   --time: 322   --train_loss: 0.7775402367115021   --train_F1: 0.7170163667167881\n",
            "Validation: \n",
            "--epoch: 4   --validation_loss: 1.1409672498703003   --validation_F1: 0.6499039375129257\n",
            "Saved best model with F1: 0.6499\n",
            "---------- Epoch:5 ----------\n",
            "Train:\n",
            "--epoch: 5   --time: 361   --train_loss: 0.6651264478762945   --train_F1: 0.7536051327678313\n",
            "Validation: \n",
            "--epoch: 5   --validation_loss: 0.5793015360832214   --validation_F1: 0.7756426870799554\n",
            "Saved best model with F1: 0.7756\n",
            "---------- Epoch:6 ----------\n",
            "Train:\n",
            "--epoch: 6   --time: 402   --train_loss: 0.49416932463645935   --train_F1: 0.7772044119508128\n",
            "Validation: \n",
            "--epoch: 6   --validation_loss: 1.470204472541809   --validation_F1: 0.7232326200496045\n",
            "---------- Epoch:7 ----------\n",
            "Train:\n",
            "--epoch: 7   --time: 434   --train_loss: 0.4895537073413531   --train_F1: 0.7708294263255983\n",
            "Validation: \n",
            "--epoch: 7   --validation_loss: 1.1305752992630005   --validation_F1: 0.7807950341714214\n",
            "Saved best model with F1: 0.7808\n",
            "---------- Epoch:8 ----------\n",
            "Train:\n",
            "--epoch: 8   --time: 470   --train_loss: 0.47389015803734463   --train_F1: 0.7879285896937477\n",
            "Validation: \n",
            "--epoch: 8   --validation_loss: 1.1872907876968384   --validation_F1: 0.6886355382905437\n",
            "---------- Epoch:9 ----------\n",
            "Train:\n",
            "--epoch: 9   --time: 502   --train_loss: 0.49102044105529785   --train_F1: 0.7737693172568157\n",
            "Validation: \n",
            "--epoch: 9   --validation_loss: 0.40194523334503174   --validation_F1: 0.8335172201065357\n",
            "Saved best model with F1: 0.8335\n",
            "---------- Epoch:10 ----------\n",
            "Train:\n",
            "--epoch: 10   --time: 532   --train_loss: 0.4407041594386101   --train_F1: 0.7980875478530431\n",
            "Validation: \n",
            "--epoch: 10   --validation_loss: 0.42559945583343506   --validation_F1: 0.8297533886438757\n",
            "---------- Epoch:11 ----------\n",
            "Train:\n",
            "--epoch: 11   --time: 561   --train_loss: 0.39784535268942517   --train_F1: 0.8233065380489525\n",
            "Validation: \n",
            "--epoch: 11   --validation_loss: 0.354806125164032   --validation_F1: 0.8506718446601942\n",
            "Saved best model with F1: 0.8507\n",
            "---------- Epoch:12 ----------\n",
            "Train:\n",
            "--epoch: 12   --time: 599   --train_loss: 0.38144025206565857   --train_F1: 0.8265350001684587\n",
            "Validation: \n",
            "--epoch: 12   --validation_loss: 0.3655738830566406   --validation_F1: 0.8525675236680899\n",
            "Saved best model with F1: 0.8526\n",
            "---------- Epoch:13 ----------\n",
            "Train:\n",
            "--epoch: 13   --time: 634   --train_loss: 0.3809683447082837   --train_F1: 0.8254601215113718\n",
            "Validation: \n",
            "--epoch: 13   --validation_loss: 0.34839189052581787   --validation_F1: 0.856039951877721\n",
            "Saved best model with F1: 0.8560\n",
            "---------- Epoch:14 ----------\n",
            "Train:\n",
            "--epoch: 14   --time: 669   --train_loss: 0.3903453101714452   --train_F1: 0.8248166710494517\n",
            "Validation: \n",
            "--epoch: 14   --validation_loss: 0.3522599935531616   --validation_F1: 0.8536942377843217\n",
            "---------- Epoch:15 ----------\n",
            "Train:\n",
            "--epoch: 15   --time: 695   --train_loss: 0.3846849550803502   --train_F1: 0.8228373045667658\n",
            "Validation: \n",
            "--epoch: 15   --validation_loss: 0.407467246055603   --validation_F1: 0.8282565249500369\n",
            "Augmented 79 images and added to the training set.\n",
            "---------- Epoch:16 ----------\n",
            "Train:\n",
            "--epoch: 16   --time: 739   --train_loss: 0.5730143758383665   --train_F1: 0.7296198901344076\n",
            "Validation: \n",
            "--epoch: 16   --validation_loss: 0.40007466077804565   --validation_F1: 0.8302476712916325\n",
            "---------- Epoch:17 ----------\n",
            "Train:\n",
            "--epoch: 17   --time: 784   --train_loss: 0.49600978466597473   --train_F1: 0.7637443433232177\n",
            "Validation: \n",
            "--epoch: 17   --validation_loss: 0.35185015201568604   --validation_F1: 0.8473832831325301\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000800 -> 0.000533\n",
            "update learning rate: 0.000800 -> 0.000533\n",
            "---------- Epoch:18 ----------\n",
            "Train:\n",
            "--epoch: 18   --time: 829   --train_loss: 0.44024716453118756   --train_F1: 0.7965851350393405\n",
            "Validation: \n",
            "--epoch: 18   --validation_loss: 0.2866067886352539   --validation_F1: 0.8866567784040345\n",
            "Saved best model with F1: 0.8867\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000533 -> 0.000356\n",
            "update learning rate: 0.000533 -> 0.000356\n",
            "---------- Epoch:19 ----------\n",
            "Train:\n",
            "--epoch: 19   --time: 879   --train_loss: 0.39663434570485895   --train_F1: 0.8185147012067439\n",
            "Validation: \n",
            "--epoch: 19   --validation_loss: 0.3378220796585083   --validation_F1: 0.8518308861432009\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000356 -> 0.000237\n",
            "update learning rate: 0.000356 -> 0.000237\n",
            "---------- Epoch:20 ----------\n",
            "Train:\n",
            "--epoch: 20   --time: 923   --train_loss: 0.3653042126785625   --train_F1: 0.8343139411637618\n",
            "Validation: \n",
            "--epoch: 20   --validation_loss: 0.26757633686065674   --validation_F1: 0.8789771901787011\n",
            "---------- Epoch:21 ----------\n",
            "Train:\n",
            "--epoch: 21   --time: 972   --train_loss: 0.3475807471708818   --train_F1: 0.8418694757442478\n",
            "Validation: \n",
            "--epoch: 21   --validation_loss: 0.23923635482788086   --validation_F1: 0.9002692298321273\n",
            "Saved best model with F1: 0.9003\n",
            "---------- Epoch:22 ----------\n",
            "Train:\n",
            "--epoch: 22   --time: 1024   --train_loss: 0.33953721956773236   --train_F1: 0.8450109347375933\n",
            "Validation: \n",
            "--epoch: 22   --validation_loss: 0.27941545844078064   --validation_F1: 0.880125766307902\n",
            "---------- Epoch:23 ----------\n",
            "Train:\n",
            "--epoch: 23   --time: 1072   --train_loss: 0.32729422775181854   --train_F1: 0.8530653843154997\n",
            "Validation: \n",
            "--epoch: 23   --validation_loss: 0.2660851776599884   --validation_F1: 0.8877184639163\n",
            "---------- Epoch:24 ----------\n",
            "Train:\n",
            "--epoch: 24   --time: 1122   --train_loss: 0.32096720012751495   --train_F1: 0.8571189830194246\n",
            "Validation: \n",
            "--epoch: 24   --validation_loss: 0.27430376410484314   --validation_F1: 0.8849684772918271\n",
            "---------- Epoch:25 ----------\n",
            "Train:\n",
            "--epoch: 25   --time: 1171   --train_loss: 0.3110788572918285   --train_F1: 0.8587036656611757\n",
            "Validation: \n",
            "--epoch: 25   --validation_loss: 0.2555033564567566   --validation_F1: 0.8874554674552689\n",
            "---------- Epoch:26 ----------\n",
            "Train:\n",
            "--epoch: 26   --time: 1220   --train_loss: 0.2919229363853281   --train_F1: 0.868762507080729\n",
            "Validation: \n",
            "--epoch: 26   --validation_loss: 0.24863001704216003   --validation_F1: 0.8923739017827875\n",
            "---------- Epoch:27 ----------\n",
            "Train:\n",
            "--epoch: 27   --time: 1268   --train_loss: 0.2916873883117329   --train_F1: 0.8687468856235516\n",
            "Validation: \n",
            "--epoch: 27   --validation_loss: 0.26683855056762695   --validation_F1: 0.8915811341711747\n",
            "---------- Epoch:28 ----------\n",
            "Train:\n",
            "--epoch: 28   --time: 1318   --train_loss: 0.2890899188139222   --train_F1: 0.869678593369672\n",
            "Validation: \n",
            "--epoch: 28   --validation_loss: 0.2842026352882385   --validation_F1: 0.8786579212916246\n",
            "---------- Epoch:29 ----------\n",
            "Train:\n",
            "--epoch: 29   --time: 1367   --train_loss: 0.2844792584126646   --train_F1: 0.8711416530644054\n",
            "Validation: \n",
            "--epoch: 29   --validation_loss: 0.2673145532608032   --validation_F1: 0.8911153631747849\n",
            "---------- Epoch:30 ----------\n",
            "Train:\n",
            "--epoch: 30   --time: 1415   --train_loss: 0.2764263620430773   --train_F1: 0.8746784346214455\n",
            "Validation: \n",
            "--epoch: 30   --validation_loss: 0.2958903908729553   --validation_F1: 0.8751315483051272\n",
            "Augmented 32 images and added to the training set.\n",
            "Threshold changed in stage 2 , set to 0.90\n",
            "---------- Epoch:31 ----------\n",
            "Train:\n",
            "--epoch: 31   --time: 1471   --train_loss: 0.34049587410229903   --train_F1: 0.8442436002547578\n",
            "Validation: \n",
            "--epoch: 31   --validation_loss: 0.24892203509807587   --validation_F1: 0.8930309821190704\n",
            "---------- Epoch:32 ----------\n",
            "Train:\n",
            "--epoch: 32   --time: 1523   --train_loss: 0.31666576633086574   --train_F1: 0.8561952044260356\n",
            "Validation: \n",
            "--epoch: 32   --validation_loss: 0.2501576542854309   --validation_F1: 0.8946015888844369\n",
            "---------- Epoch:33 ----------\n",
            "Train:\n",
            "--epoch: 33   --time: 1575   --train_loss: 0.3018633872270584   --train_F1: 0.8626192258544303\n",
            "Validation: \n",
            "--epoch: 33   --validation_loss: 0.24916955828666687   --validation_F1: 0.8961491868874124\n",
            "---------- Epoch:34 ----------\n",
            "Train:\n",
            "--epoch: 34   --time: 1628   --train_loss: 0.30329741480258793   --train_F1: 0.8621659133176343\n",
            "Validation: \n",
            "--epoch: 34   --validation_loss: 0.2252589762210846   --validation_F1: 0.9046505325666887\n",
            "Saved best model with F1: 0.9047\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000237 -> 0.000158\n",
            "update learning rate: 0.000237 -> 0.000158\n",
            "---------- Epoch:35 ----------\n",
            "Train:\n",
            "--epoch: 35   --time: 1683   --train_loss: 0.2834649682044983   --train_F1: 0.871415848229079\n",
            "Validation: \n",
            "--epoch: 35   --validation_loss: 0.2360205054283142   --validation_F1: 0.8972526560164922\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000158 -> 0.000105\n",
            "update learning rate: 0.000158 -> 0.000105\n",
            "---------- Epoch:36 ----------\n",
            "Train:\n",
            "--epoch: 36   --time: 1736   --train_loss: 0.2553462346012776   --train_F1: 0.8844909938314649\n",
            "Validation: \n",
            "--epoch: 36   --validation_loss: 0.24349108338356018   --validation_F1: 0.896453865857479\n",
            "---------- Epoch:37 ----------\n",
            "Train:\n",
            "--epoch: 37   --time: 1794   --train_loss: 0.24546123353334573   --train_F1: 0.8893561811676215\n",
            "Validation: \n",
            "--epoch: 37   --validation_loss: 0.2101237028837204   --validation_F1: 0.9100110247314104\n",
            "Saved best model with F1: 0.9100\n",
            "---------- Epoch:38 ----------\n",
            "Train:\n",
            "--epoch: 38   --time: 1855   --train_loss: 0.23992148156349474   --train_F1: 0.8910544964219463\n",
            "Validation: \n",
            "--epoch: 38   --validation_loss: 0.22338849306106567   --validation_F1: 0.9085849384768676\n",
            "---------- Epoch:39 ----------\n",
            "Train:\n",
            "--epoch: 39   --time: 1913   --train_loss: 0.23786917042273742   --train_F1: 0.8915604279834326\n",
            "Validation: \n",
            "--epoch: 39   --validation_loss: 0.226791113615036   --validation_F1: 0.903208026635818\n",
            "---------- Epoch:40 ----------\n",
            "Train:\n",
            "--epoch: 40   --time: 1974   --train_loss: 0.2267174726495376   --train_F1: 0.8966536610550536\n",
            "Validation: \n",
            "--epoch: 40   --validation_loss: 0.23803484439849854   --validation_F1: 0.8999802564197485\n",
            "---------- Epoch:41 ----------\n",
            "Train:\n",
            "--epoch: 41   --time: 2030   --train_loss: 0.21913143419302428   --train_F1: 0.9005833010152832\n",
            "Validation: \n",
            "--epoch: 41   --validation_loss: 0.23252594470977783   --validation_F1: 0.9060911294499077\n",
            "---------- Epoch:42 ----------\n",
            "Train:\n",
            "--epoch: 42   --time: 2087   --train_loss: 0.23071893877707994   --train_F1: 0.8945420241775366\n",
            "Validation: \n",
            "--epoch: 42   --validation_loss: 0.22793887555599213   --validation_F1: 0.9083928618422015\n",
            "---------- Epoch:43 ----------\n",
            "Train:\n",
            "--epoch: 43   --time: 2140   --train_loss: 0.20773047151473853   --train_F1: 0.9055417665760626\n",
            "Validation: \n",
            "--epoch: 43   --validation_loss: 0.22372408211231232   --validation_F1: 0.9015655590465362\n",
            "---------- Epoch:44 ----------\n",
            "Train:\n",
            "--epoch: 44   --time: 2195   --train_loss: 0.2057312302864515   --train_F1: 0.9073345637309741\n",
            "Validation: \n",
            "--epoch: 44   --validation_loss: 0.2348167598247528   --validation_F1: 0.8995029394696339\n",
            "---------- Epoch:45 ----------\n",
            "Train:\n",
            "--epoch: 45   --time: 2250   --train_loss: 0.20222772600559089   --train_F1: 0.9078007677398614\n",
            "Validation: \n",
            "--epoch: 45   --validation_loss: 0.22338713705539703   --validation_F1: 0.9091042459487294\n",
            "Augmented 54 images and added to the training set.\n",
            "---------- Epoch:46 ----------\n",
            "Train:\n",
            "--epoch: 46   --time: 2321   --train_loss: 0.3069979805837978   --train_F1: 0.8613690191883184\n",
            "Validation: \n",
            "--epoch: 46   --validation_loss: 0.2089579850435257   --validation_F1: 0.9133913627794404\n",
            "Saved best model with F1: 0.9134\n",
            "---------- Epoch:47 ----------\n",
            "Train:\n",
            "--epoch: 47   --time: 2390   --train_loss: 0.2719097855416211   --train_F1: 0.8769210397263718\n",
            "Validation: \n",
            "--epoch: 47   --validation_loss: 0.2249811589717865   --validation_F1: 0.904989792242104\n",
            "---------- Epoch:48 ----------\n",
            "Train:\n",
            "--epoch: 48   --time: 2454   --train_loss: 0.25208910170829657   --train_F1: 0.8871747815658771\n",
            "Validation: \n",
            "--epoch: 48   --validation_loss: 0.20412394404411316   --validation_F1: 0.9129793510324484\n",
            "---------- Epoch:49 ----------\n",
            "Train:\n",
            "--epoch: 49   --time: 2520   --train_loss: 0.24401638498812012   --train_F1: 0.8894777443316684\n",
            "Validation: \n",
            "--epoch: 49   --validation_loss: 0.2306685894727707   --validation_F1: 0.9045372480539853\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000105 -> 0.000070\n",
            "update learning rate: 0.000105 -> 0.000070\n",
            "---------- Epoch:50 ----------\n",
            "Train:\n",
            "--epoch: 50   --time: 2585   --train_loss: 0.22375890522292166   --train_F1: 0.8985812151839428\n",
            "Validation: \n",
            "--epoch: 50   --validation_loss: 0.2171379029750824   --validation_F1: 0.9096659550725341\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000070 -> 0.000047\n",
            "update learning rate: 0.000070 -> 0.000047\n",
            "---------- Epoch:51 ----------\n",
            "Train:\n",
            "--epoch: 51   --time: 2649   --train_loss: 0.20689493056499597   --train_F1: 0.9075068064944732\n",
            "Validation: \n",
            "--epoch: 51   --validation_loss: 0.22960683703422546   --validation_F1: 0.9052438947812228\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000047 -> 0.000031\n",
            "update learning rate: 0.000047 -> 0.000031\n",
            "---------- Epoch:52 ----------\n",
            "Train:\n",
            "--epoch: 52   --time: 2714   --train_loss: 0.1984066723874121   --train_F1: 0.910711469256321\n",
            "Validation: \n",
            "--epoch: 52   --validation_loss: 0.2104034721851349   --validation_F1: 0.910645870027055\n",
            "---------- Epoch:53 ----------\n",
            "Train:\n",
            "--epoch: 53   --time: 2782   --train_loss: 0.19881034529570377   --train_F1: 0.9096450850879261\n",
            "Validation: \n",
            "--epoch: 53   --validation_loss: 0.2365948110818863   --validation_F1: 0.9054749788461309\n",
            "---------- Epoch:54 ----------\n",
            "Train:\n",
            "--epoch: 54   --time: 2847   --train_loss: 0.19186683586149506   --train_F1: 0.9126481546645435\n",
            "Validation: \n",
            "--epoch: 54   --validation_loss: 0.20961812138557434   --validation_F1: 0.9114330418087407\n",
            "---------- Epoch:55 ----------\n",
            "Train:\n",
            "--epoch: 55   --time: 2915   --train_loss: 0.19088581430189538   --train_F1: 0.9138627059748101\n",
            "Validation: \n",
            "--epoch: 55   --validation_loss: 0.23603521287441254   --validation_F1: 0.902726699590207\n",
            "---------- Epoch:56 ----------\n",
            "Train:\n",
            "--epoch: 56   --time: 2983   --train_loss: 0.18374963827205426   --train_F1: 0.9174290027931052\n",
            "Validation: \n",
            "--epoch: 56   --validation_loss: 0.2123815417289734   --validation_F1: 0.9175066001760047\n",
            "Saved best model with F1: 0.9175\n",
            "---------- Epoch:57 ----------\n",
            "Train:\n",
            "--epoch: 57   --time: 3061   --train_loss: 0.1832300876126145   --train_F1: 0.9176992973027851\n",
            "Validation: \n",
            "--epoch: 57   --validation_loss: 0.22055724263191223   --validation_F1: 0.9119935573267643\n",
            "---------- Epoch:58 ----------\n",
            "Train:\n",
            "--epoch: 58   --time: 3129   --train_loss: 0.17840440074602762   --train_F1: 0.9201549798674239\n",
            "Validation: \n",
            "--epoch: 58   --validation_loss: 0.2172878384590149   --validation_F1: 0.9088687526270002\n",
            "---------- Epoch:59 ----------\n",
            "Train:\n",
            "--epoch: 59   --time: 3199   --train_loss: 0.17529902697512598   --train_F1: 0.9212509500147122\n",
            "Validation: \n",
            "--epoch: 59   --validation_loss: 0.20488019287586212   --validation_F1: 0.9104027390093608\n",
            "---------- Epoch:60 ----------\n",
            "Train:\n",
            "--epoch: 60   --time: 3271   --train_loss: 0.17094649058399777   --train_F1: 0.9226812733364302\n",
            "Validation: \n",
            "--epoch: 60   --validation_loss: 0.20694869756698608   --validation_F1: 0.9164359135086448\n",
            "Augmented 24 images and added to the training set.\n",
            "---------- Epoch:61 ----------\n",
            "Train:\n",
            "--epoch: 61   --time: 3351   --train_loss: 0.2073519238167339   --train_F1: 0.9065479676686812\n",
            "Validation: \n",
            "--epoch: 61   --validation_loss: 0.21161888539791107   --validation_F1: 0.9118692123291838\n",
            "---------- Epoch:62 ----------\n",
            "Train:\n",
            "--epoch: 62   --time: 3421   --train_loss: 0.19293000797430673   --train_F1: 0.9138696733329914\n",
            "Validation: \n",
            "--epoch: 62   --validation_loss: 0.2120736837387085   --validation_F1: 0.9143284902927039\n",
            "---------- Epoch:63 ----------\n",
            "Train:\n",
            "--epoch: 63   --time: 3490   --train_loss: 0.18349458049568865   --train_F1: 0.9178272805260606\n",
            "Validation: \n",
            "--epoch: 63   --validation_loss: 0.2475537657737732   --validation_F1: 0.899012359756986\n",
            "---------- Epoch:64 ----------\n",
            "Train:\n",
            "--epoch: 64   --time: 3560   --train_loss: 0.18000625798271763   --train_F1: 0.9188273510910664\n",
            "Validation: \n",
            "--epoch: 64   --validation_loss: 0.22089043259620667   --validation_F1: 0.9105279929477844\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000031 -> 0.000021\n",
            "update learning rate: 0.000031 -> 0.000021\n",
            "---------- Epoch:65 ----------\n",
            "Train:\n",
            "--epoch: 65   --time: 3630   --train_loss: 0.17742764494485325   --train_F1: 0.9212716231196934\n",
            "Validation: \n",
            "--epoch: 65   --validation_loss: 0.20685768127441406   --validation_F1: 0.9132469322071203\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000021 -> 0.000014\n",
            "update learning rate: 0.000021 -> 0.000014\n",
            "---------- Epoch:66 ----------\n",
            "Train:\n",
            "--epoch: 66   --time: 3699   --train_loss: 0.17029638981653583   --train_F1: 0.9234903274069648\n",
            "Validation: \n",
            "--epoch: 66   --validation_loss: 0.197271466255188   --validation_F1: 0.9205130831380252\n",
            "Saved best model with F1: 0.9205\n",
            "---------- Epoch:67 ----------\n",
            "Train:\n",
            "--epoch: 67   --time: 3785   --train_loss: 0.1672150395396683   --train_F1: 0.9250630965996821\n",
            "Validation: \n",
            "--epoch: 67   --validation_loss: 0.21019214391708374   --validation_F1: 0.9112481409067362\n",
            "---------- Epoch:68 ----------\n",
            "Train:\n",
            "--epoch: 68   --time: 3861   --train_loss: 0.16138939207626712   --train_F1: 0.927440571623081\n",
            "Validation: \n",
            "--epoch: 68   --validation_loss: 0.22765794396400452   --validation_F1: 0.9048638049918084\n",
            "---------- Epoch:69 ----------\n",
            "Train:\n",
            "--epoch: 69   --time: 3939   --train_loss: 0.16526641448338827   --train_F1: 0.9246265899537006\n",
            "Validation: \n",
            "--epoch: 69   --validation_loss: 0.24656523764133453   --validation_F1: 0.9059138505473584\n",
            "---------- Epoch:70 ----------\n",
            "Train:\n",
            "--epoch: 70   --time: 4008   --train_loss: 0.1612311431931125   --train_F1: 0.9269043901992584\n",
            "Validation: \n",
            "--epoch: 70   --validation_loss: 0.23521798849105835   --validation_F1: 0.90168093777626\n",
            "---------- Epoch:71 ----------\n",
            "Train:\n",
            "--epoch: 71   --time: 4084   --train_loss: 0.1583988465782669   --train_F1: 0.9287685622279742\n",
            "Validation: \n",
            "--epoch: 71   --validation_loss: 0.22712549567222595   --validation_F1: 0.9106970936440604\n",
            "---------- Epoch:72 ----------\n",
            "Train:\n",
            "--epoch: 72   --time: 4157   --train_loss: 0.1589250322431326   --train_F1: 0.9281954827974737\n",
            "Validation: \n",
            "--epoch: 72   --validation_loss: 0.21219398081302643   --validation_F1: 0.9131725310509784\n",
            "---------- Epoch:73 ----------\n",
            "Train:\n",
            "--epoch: 73   --time: 4227   --train_loss: 0.15264005276064077   --train_F1: 0.9313374305933844\n",
            "Validation: \n",
            "--epoch: 73   --validation_loss: 0.24889658391475677   --validation_F1: 0.9049565377672321\n",
            "---------- Epoch:74 ----------\n",
            "Train:\n",
            "--epoch: 74   --time: 4300   --train_loss: 0.14828291535377502   --train_F1: 0.9335284250524476\n",
            "Validation: \n",
            "--epoch: 74   --validation_loss: 0.2272694706916809   --validation_F1: 0.9039225203683735\n",
            "---------- Epoch:75 ----------\n",
            "Train:\n",
            "--epoch: 75   --time: 4375   --train_loss: 0.15121308010485437   --train_F1: 0.9317895458235793\n",
            "Validation: \n",
            "--epoch: 75   --validation_loss: 0.19918416440486908   --validation_F1: 0.9161703987319892\n",
            "Augmented 8 images and added to the training set.\n",
            "---------- Epoch:76 ----------\n",
            "Train:\n",
            "--epoch: 76   --time: 4447   --train_loss: 0.16505812773027936   --train_F1: 0.9259737438528408\n",
            "Validation: \n",
            "--epoch: 76   --validation_loss: 0.20232491195201874   --validation_F1: 0.9222149631928322\n",
            "Saved best model with F1: 0.9222\n",
            "---------- Epoch:77 ----------\n",
            "Train:\n",
            "--epoch: 77   --time: 4526   --train_loss: 0.15909316652529948   --train_F1: 0.9293617908713335\n",
            "Validation: \n",
            "--epoch: 77   --validation_loss: 0.2272133082151413   --validation_F1: 0.9094632527319912\n",
            "---------- Epoch:78 ----------\n",
            "Train:\n",
            "--epoch: 78   --time: 4598   --train_loss: 0.16014930223290985   --train_F1: 0.9288529879719565\n",
            "Validation: \n",
            "--epoch: 78   --validation_loss: 0.24152886867523193   --validation_F1: 0.9027557845477535\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000014 -> 0.000009\n",
            "update learning rate: 0.000014 -> 0.000009\n",
            "---------- Epoch:79 ----------\n",
            "Train:\n",
            "--epoch: 79   --time: 4670   --train_loss: 0.1588631971059619   --train_F1: 0.9283391775813036\n",
            "Validation: \n",
            "--epoch: 79   --validation_loss: 0.2322109192609787   --validation_F1: 0.9117300148237891\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000009 -> 0.000006\n",
            "update learning rate: 0.000009 -> 0.000006\n",
            "---------- Epoch:80 ----------\n",
            "Train:\n",
            "--epoch: 80   --time: 4741   --train_loss: 0.14904671424144023   --train_F1: 0.9334578578065181\n",
            "Validation: \n",
            "--epoch: 80   --validation_loss: 0.24697409570217133   --validation_F1: 0.8995510963547507\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000006 -> 0.000004\n",
            "update learning rate: 0.000006 -> 0.000004\n",
            "---------- Epoch:81 ----------\n",
            "Train:\n",
            "--epoch: 81   --time: 4814   --train_loss: 0.15161158768711863   --train_F1: 0.9321880447524462\n",
            "Validation: \n",
            "--epoch: 81   --validation_loss: 0.24080896377563477   --validation_F1: 0.9044011175533992\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000004 -> 0.000003\n",
            "update learning rate: 0.000004 -> 0.000003\n",
            "---------- Epoch:82 ----------\n",
            "Train:\n",
            "--epoch: 82   --time: 4885   --train_loss: 0.14881710788688143   --train_F1: 0.9328770605263943\n",
            "Validation: \n",
            "--epoch: 82   --validation_loss: 0.21641667187213898   --validation_F1: 0.9119125237707145\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000003 -> 0.000002\n",
            "update learning rate: 0.000003 -> 0.000002\n",
            "---------- Epoch:83 ----------\n",
            "Train:\n",
            "--epoch: 83   --time: 4957   --train_loss: 0.15005857114856308   --train_F1: 0.9325880868531884\n",
            "Validation: \n",
            "--epoch: 83   --validation_loss: 0.23326542973518372   --validation_F1: 0.9043529206452096\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000002 -> 0.000001\n",
            "update learning rate: 0.000002 -> 0.000001\n",
            "---------- Epoch:84 ----------\n",
            "Train:\n",
            "--epoch: 84   --time: 5029   --train_loss: 0.15022765039592176   --train_F1: 0.9332415530243868\n",
            "Validation: \n",
            "--epoch: 84   --validation_loss: 0.23402929306030273   --validation_F1: 0.9086390344140357\n",
            "<_io.TextIOWrapper name='logs/DinkNet152_8_2e_4.log' mode='w' encoding='UTF-8'> update learning rate: 0.000001 -> 0.000001\n",
            "update learning rate: 0.000001 -> 0.000001\n",
            "---------- Epoch:85 ----------\n",
            "Train:\n",
            "--epoch: 85   --time: 5100   --train_loss: 0.1576721853501088   --train_F1: 0.9309324475975777\n",
            "Validation: \n",
            "--epoch: 85   --validation_loss: 0.22212836146354675   --validation_F1: 0.9147749027447748\n",
            "early stop at 10 epoch\n",
            "Threshold changed in stage 6 , set to 0.94\n",
            "---------- Epoch:91 ----------\n",
            "Train:\n",
            "--epoch: 91   --time: 5172   --train_loss: 0.1485498462979858   --train_F1: 0.9342433319319664\n",
            "Validation: \n",
            "--epoch: 91   --validation_loss: 0.2081286609172821   --validation_F1: 0.9159095615261169\n",
            "early stop at 1 epoch\n",
            "---------- Epoch:106 ----------\n",
            "Train:\n",
            "--epoch: 106   --time: 5244   --train_loss: 0.15263124092205152   --train_F1: 0.9317938151651661\n",
            "Validation: \n",
            "--epoch: 106   --validation_loss: 0.25647538900375366   --validation_F1: 0.9004343038903841\n",
            "early stop at 1 epoch\n",
            "Finish!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the Results"
      ],
      "metadata": {
        "id": "Bzlemusu-JYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(range(10, len(train_loss_list) + 10), train_loss_list, label=\"Training Loss\", color='blue')\n",
        "plt.plot(range(10, len(val_loss_list) + 10), val_loss_list, label=\"Validation Loss\", color='orange')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title(\"Training and Validation Loss Over Epochs\", fontsize=16)\n",
        "plt.xlabel(\"Epochs\", fontsize=14)\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.savefig('loss_over_epochs.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "28mSBhaEUtWo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "b7e60822-8c42-4153-815a-f97f5d706ea8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Accuracy"
      ],
      "metadata": {
        "id": "mSJu7m3a-ByO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rhl_sL9andLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e9880f-51a5-48c2-f73a-10091e97bc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-240879952428>:61: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  self.img = V(self.img.to(self.device), volatile=volatile)\n",
            "<ipython-input-3-240879952428>:63: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  self.mask = V(self.mask.to(self.device), volatile=volatile)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9589\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "\n",
        "solver.net.eval()\n",
        "\n",
        "val_accuracy = 0\n",
        "\n",
        "\n",
        "val_data_loader_iter = iter(val_loader)\n",
        "for val_img, val_mask in val_data_loader_iter:\n",
        "\n",
        "    solver.set_input(val_img, val_mask)\n",
        "\n",
        "    predicted_mask, _ = solver.test_batch()\n",
        "    predicted_mask = (predicted_mask > 0.5).astype(np.float32)\n",
        "\n",
        "\n",
        "    predicted_mask = torch.tensor(predicted_mask).to(torch.float32)\n",
        "\n",
        "\n",
        "    val_accuracy += accuracy_score(val_mask.cpu().numpy().flatten(),\n",
        "                                   predicted_mask.cpu().numpy().flatten())\n",
        "\n",
        "val_accuracy /= len(val_data_loader_iter)\n",
        "\n",
        "\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the files for submition"
      ],
      "metadata": {
        "id": "xjzwauJU9ozx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test() # calling the function that predicts the test_set_images file and gives us a label for each of them"
      ],
      "metadata": {
        "id": "YF2-4I4t-TwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdf79f8-f7f4-40d3-b952-69713a574e97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-6-199b312c222f>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.net.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0      0.00\n",
            "Testing on  test_1.png\n",
            "Testing on  test_10.png\n",
            "Testing on  test_11.png\n",
            "Testing on  test_12.png\n",
            "Testing on  test_13.png\n",
            "Testing on  test_14.png\n",
            "Testing on  test_15.png\n",
            "Testing on  test_16.png\n",
            "Testing on  test_17.png\n",
            "Testing on  test_18.png\n",
            "1.0      57.41\n",
            "Testing on  test_19.png\n",
            "Testing on  test_2.png\n",
            "Testing on  test_20.png\n",
            "Testing on  test_21.png\n",
            "Testing on  test_22.png\n",
            "Testing on  test_23.png\n",
            "Testing on  test_24.png\n",
            "Testing on  test_25.png\n",
            "Testing on  test_26.png\n",
            "Testing on  test_27.png\n",
            "2.0      115.43\n",
            "Testing on  test_28.png\n",
            "Testing on  test_29.png\n",
            "Testing on  test_3.png\n",
            "Testing on  test_30.png\n",
            "Testing on  test_31.png\n",
            "Testing on  test_32.png\n",
            "Testing on  test_33.png\n",
            "Testing on  test_34.png\n",
            "Testing on  test_35.png\n",
            "Testing on  test_36.png\n",
            "3.0      172.13\n",
            "Testing on  test_37.png\n",
            "Testing on  test_38.png\n",
            "Testing on  test_39.png\n",
            "Testing on  test_4.png\n",
            "Testing on  test_40.png\n",
            "Testing on  test_41.png\n",
            "Testing on  test_42.png\n",
            "Testing on  test_43.png\n",
            "Testing on  test_44.png\n",
            "Testing on  test_45.png\n",
            "4.0      228.05\n",
            "Testing on  test_46.png\n",
            "Testing on  test_47.png\n",
            "Testing on  test_48.png\n",
            "Testing on  test_49.png\n",
            "Testing on  test_5.png\n",
            "Testing on  test_50.png\n",
            "Testing on  test_6.png\n",
            "Testing on  test_7.png\n",
            "Testing on  test_8.png\n",
            "Testing on  test_9.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-wDAv-82qEXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import re\n",
        "\n",
        "foreground_threshold = 0.25\n",
        "\n",
        "def patch_to_label(patch):\n",
        "    df = np.mean(patch)\n",
        "    if df > foreground_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def mask_to_submission_strings(full_path, image_filename):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
        "    im = mpimg.imread(full_path)\n",
        "    patch_size = 16\n",
        "    for j in range(0, im.shape[1], patch_size):\n",
        "        for i in range(0, im.shape[0], patch_size):\n",
        "            patch = im[i:i + patch_size, j:j + patch_size]\n",
        "            label = patch_to_label(patch)\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
        "\n",
        "\n",
        "def masks_to_submission(submission_filename, submission_mask_path, image_filenames):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for fn in image_filenames[0:]:\n",
        "            path = os.path.join(submission_mask_path, fn)\n",
        "            f.writelines('{}\\n'.format(s)\n",
        "                         for s in mask_to_submission_strings(path, fn))\n",
        "\n",
        "\n",
        "def submit():\n",
        "    submission_filename = 'DinkNet152.csv'\n",
        "    submission_mask_path = 'submits/DinkNet152/'\n",
        "    end = int(len('_mask.png'))\n",
        "    start = int(len('test_'))\n",
        "    image_names = os.listdir(submission_mask_path)\n",
        "    image_names.sort(key=lambda x: int(x[start:][:-end]))\n",
        "    masks_to_submission(submission_filename, submission_mask_path, image_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JxNBAoqzqFic"
      },
      "outputs": [],
      "source": [
        "submit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}